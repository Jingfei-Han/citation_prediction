This project is applied for data crawler from google scholar. And the next time we will predict the number of paper's citation.

Fllowing is my work everytime.

----------------------------------2017.4.10------------------------------
使用谷歌学术镜像网站
https://a.ggkai.men/extdomains/scholar.google.com/
设置内容为英文显示，每次显示20行

给出两篇论文的url格式：
1: Blowing in the wind: unanchored patient information work during cancer care

url1: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=Blowing+in+the+wind%3A+unanchored+patient+information+work+during+cancer+care&btnG=&as_sdt=1%2C5&as_sdtp= 

2: I don't mind being logged, but want to remain in control: a field study of mobile activity and context logging     
url2: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=I+don%27t+mind+being+logged%2C+but+want+to+remain+in+control%3A+a+field+study+of+mobile+activity+and+context+logging&btnG=&as_sdt=1%2C5&as_sdtp=

可以发现假设论文名字为PAPER NAME, url格式为：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=PAPER+NAME&btnG=&as_sdt=1%2C5&as_sdtp=

一定会存在一些论文在google学术里面无法找到，因此需要抓住关键信息，判断是否可以在google学术中搜索到。
使用requests, BeautifulSoup进行测试，如下：
response = requests.get(url,headers=headers)
soup = BeautifulSoup(response.text)

linkinfo = soup.find("div", {"class":"gs_a"}).get_text()
可以得到linkinfo是google学术条目的作者信息那一行，我们可以使用作者信息判断该论文是否可以在谷歌学术中找到。

问题：作者名字存在法语或其他特殊字符
解决：目前不需要该作者信息

-----------------------------------2017.4.11---------------------------------
写一个小demo测试爬虫是否正确
镜像网站不稳定，因此使用vpn访问https://scholar.google.com
记录headers:

cookie = 'NID=101=SZYmK1bDCNI9YVEM-lBxM975ArpgyelHkwNMiiCJVjoY4sbhBGUWJ-zzrlo2_r1-8LeeeavR1hn8UxP2MuAM92L-uWOzdhExx-OIZZhuVlAGDS6P7XpR15PzPlcPSErq; GSP=IN=7e6cc990821af63:LD=en:NR=20:LM=1491915063:S=OmZJEGX4GZgsoDj5'
headers = {
	'Host': 'scholar.google.com',
	'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
	'Accept-Encoding':'gzip, deflate, sdch, br',
	'Accept-Language':'zh-CN,en-US;q=0.8,zh;q=0.5,en;q=0.3',
	'Cookie': cookie,
	'Referer':'https://www.google.com',
	'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
	'Cache-Control':'max-age=0',
}


问题：google学术使用requests.utils.dict_from_cookiejar(response.cookies)没有cookie返回。
待解决。

---------------------------------2017.04.13------------------------------------
首先测试两次无headers爬取google的cookie分别是什么？
第一次访问：
https://scholar.google.com/scholar?hl=en&q=Yoshua+Bengio&as_sdt=1%2C5&as_sdtp=&oq=
最终cookie为：
'GSP': 'LM=1492085690:S=MT2DJ2tBDnQA2lmr'
'NID': '101=fSdLNDYt8Adg6J2jqn2zV1FLD1VtFzPPA1tZM52d7DWDhMxo8M43HgTBZ6Uw-54Hn5el6rJmHrJPjHbCdzBo1B2PpDGHoB2zLaZ3uOrz7TvoCt8EY_rQGxgK6t9xNtsH'

第二次访问：
https://scholar.google.com/scholar?q=computer+network+security&hl=en&as_sdt=0%2C5&oq=
最终cookie为：
'GSP': 'LM=1492085859:S=R3n1bWo54jcZMaNf',
'NID': '101=GascO37p3VoBYkwSB43S9c-Vuoo2Q4MoMCGFeW5E8OdG3Fmxh1BYhvyGeF2a37om1B-DMhhRmVSrWq_AyF6vmcVSGjz9Av0SACHAVsIU4b1DQucOUXaxTPzl8aDpzyUw'

决定继续写test爬虫，这次采用手工获取cookie和user-agent的方式，明天写代码。

---------------------------------2017.04.14------------------------------------
发现一个python库，user_agent, 可以生成user_agent,安装如下：
pip install -U user_agent
使用方法：
from user_agent import generate_user_agent
Agent = generate_user_agent()

---------------------------------2017.04.15------------------------------------
cookie不易获取， 准备尝试先爬代理， 然后用代理爬国内google学术的镜像。
发现一个requests的关键属性,比如：res = requests.get(url)
res.status_code为状态码，200为正常（requests.codes.ok）

从西刺代理网站爬取500个代理IP和端口号
问题：很多代理无法ping通。

---------------------------------2017.04.16------------------------------------
对Crawl_proxy:
写一个函数用于判断是否ip可以ping通，只有能够ping通的才会被写入proxies.txt中

问题：cmd下运行python出现SyntaxError错误
解决：因为有中文注释，第一行加上#encoding:utf-8

问题：代理用不了，报错：
 ProxyError: HTTPSConnectionPool(host='b.ggkai.men', port=443): Max retries exceeded with url: /extdomains/scholar.google.com/scholar?hl=en&q=Usable+gestures+for+mobile+interfaces%3A+evaluating+social+acceptability+&btnG=&as_sdt=1%2C5&as_sdtp= (Caused by ProxyError('Cannot connect to proxy.', error(10054, '')))


代理发现镜像网站好像没有反爬虫，先测试8000个左右例子，看是否有问题
发现大概爬100个左右会出现重定向次数过多的情况，需要清除cookie

梳理下现在的有待解决的问题：
1. 数据库表结构优化
2. paper加上一些属性，比如引用的url等
3. 数据库表内容的优化（直接去掉不必要内容）
4. dblp名字变更问题
5. 二级爬虫程序未写

在爬镜像网站时在headers中加入cookie，可以连续爬取，目前到第262个遇到问题，原因是ascii编码的问题
问题： 对于论文Designing urban media façades: cases and challenges，其中包括非ascii编码的字符
解决：编码问题，可以设置默认编码， 方式如下：
import sys
reload(sys)
sys.setdefaultencoding("utf-8")

demo跑了1056次，我主动停了demo，准备更新数据库。
先在demo中运行，首先在test_paper表中加入两列属性，一个是引用链接，另一个是pdf链接

Mysql语句为：
ALTER TABLE test_citation.test_paper ADD COLUMN paper_citationURL VARCHAR(2550);
ALTER TABLE test_citation.test_paper ADD COLUMN paper_pdfURL VARCHAR(2550);

添加Crawl_paper，用于真实数据库爬虫

-------------------------------------------2017.04.17------------------------------------------------
本来准备将所有论文的引用全部爬下来， 现在来看可以分开来爬， 按类来爬。 SQL程序如下：
#找出所有A类会议、未被爬过的的论文名
SELECT paper_id, paper_title, paper_nbCitation, paper_isseen
FROM citation.paper
WHERE venue_venue_id IN(
	SELECT venue_id#, venue_name, venue_dblpname
	FROM citation.venue
	WHERE dblp_dblp_id IN (
		SELECT dblp_id
		FROM citation.ccf, citation.dblp
		WHERE CCF_dblpname = dblp_name
		AND CCF_type = 'conference'
		AND CCF_classification = 'A'
	)
)
AND paper_nbCitation != -1

这里参数就是CCF_type 和CCF_classification

--------------------------------------------2017.04.17-----------------------------------------------
问题：爬虫程序阻塞，但是不会停止运行，这导致无法连续爬虫
解决：编写定时重启程序的脚本，主要是两步：杀死进程、启动进程
代码见code/restart.py


---------------------------------------------2017.04.19----------------------------------------------
问题：发现爬虫解析网页内容有逻辑问题。目前发现很多pdf链接爬取错误。主要原因是如果显示超过一个结果，则会找到第一个有pdf的pdf链接（即不一定是第一个）因此导致错误。虽然没有看引用数是否有问题，但是感觉逻辑上也可能有问题。
解决：暂时未解决。

现在工作大概有三部分：数据库、DBLP、Google Scholar
数据库：可能数据从Aminer移植过程中，由于去噪等逻辑漏洞，导致移植出错
DBLP：一文多投、一刊多名等问题
Scholar： 逻辑问题，try except块应该保证逻辑正确，可以多输出中间信息

同时现在脚本运行的日志记录有问题，比如第几条成功的情况下是不用记录的

准备将这三部分的数据错误问题解决一下再继续后面的工作。
首先是数据库
将钱学长给的全部数据的数据库citation_raw与我们数据库citation做比较，SQL代码如下：
SELECT citation.paper.paper_id, citation.paper.paper_title, citation.venue.venue_name, citation_raw.paper.paper_venue_name
FROM citation.paper, citation.venue, citation_raw.paper
WHERE citation.paper.paper_id = citation_raw.paper.paper_id
AND citation.paper.venue_venue_id = citation.venue.venue_id
AND citation.venue.venue_name != citation_raw.paper.paper_venue_name

最终得到结果，有486篇论文的venue_name不一样，使用Aminer数据库进行比较，最终得到：
####################例子1###################
'1401710', 'Panlingual lexical translation via probabilistic inference', 'Artificial Intelligence', 'Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010'

citation_raw：Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010
citation: Artificial Intelligence
AMiner : Artificial Intelligence
证明我们的数据库没有问题。
究其原因，我将该论文：Panlingual lexical translation via probabilistic inference放到DBLP数据库中搜索，得到结果如下：
found 2 matches
2010
	Mausam, Stephen Soderland, Oren Etzioni, Daniel S. Weld, Kobi Reiter, Michael Skinner, Marcus Sammer, Jeff A. Bilmes:
Panlingual lexical translation via probabilistic inference. Artif. Intell. 174(9-10): 619-637 (2010)
	Mausam, Stephen Soderland, Oren Etzioni:
Panlingual Lexical Translation via Probabilistic Inference. AAAI 2010
maintained by Schloss Dagstuhl LZI at University of Trier	homebrowsesearchabout
open data data released under the ODC-BY 1.0 license; see also our legal information page

可以发现该论文在2010年被放到了AI期刊和AIII会议。
##################例子2######################
'2078528', 'Proceedings of the 15th Eurographics Conference on Visualization', 'EuroVis \'13 Proceedings of the 15th Eurographics Conference on Visualization', 'Computer Graphics Forum'

citation_raw : Computer Graphics Forum
citation : EuroVis '13 Proceedings of the 15th Eurographics Conference on Visualization
AMiner : Proceedings of the 15th Eurographics Conference on Visualization
citation是正确的，并且citation在插入数据过程中进行了一定的正则处理

#################例子3#########################
'1381004', 'Decoupled Linear Estimation of Affine Geometric Deformations and Nonlinear Intensity Transformations of Images', '', 'IEEE Transactions on Pattern Analysis and Machine Intelligence'

citation_raw : IEEE Transactions on Pattern Analysis and Machine Intelligence
citation : 空
AMiner : 空
citation是正确的

#############################################
总结：
我们发现citation在venue栏应该是没有问题的， 学长的citation_raw应该是采用了别的方法得到的venue。
下面构建一个AMiner数据库，其中就包括两个表，一个是paper表，一个是author表。
从citation中把表结构复制过来（空表）, SQL代码如下
CREATE TABLE aminer.paper LIKE citation.paper;
CREATE TABLE aminer.author LIKE citation.author;

结合程序Insert_Paper.py 和 Insert_Author2.py来编写程序，将所有aminer信息原封不动写入aminer数据库
需要添加venue_name列，移出一些列，SQL代码如下：

#删除列
ALTER TABLE aminer.paper DROP COLUMN venue_venue_id;
ALTER TABLE aminer.paper DROP COLUMN paper_isseen;
ALTER TABLE aminer.paper DROP COLUMN paper_citationURL;
ALTER TABLE aminer.paper DROP COLUMN paper_pdfURL;
ALTER TABLE aminer.paper DROP COLUMN paper_nbCitation;
#添加列
ALTER TABLE aminer.paper ADD COLUMN paper_venuename VARCHAR(2550);

下面准备把citation数据库中author 和 a2p加入数据库

------------------------------------2017.04.20---------------------------------
讨论目前存在的问题：
CCF的DBLP名的爬虫问题：
1. Likely matches, 有些情况是venue换名字，有些匹配的是错的，需要判断。但是Exact match是一定对的。
2. Likely matches情况，如果venue换名字，应在mysql中加上两列，记录曾用名。
3. 期刊的dblp名不是简称，但会议的dblp名应该是简称，需要再验证下。但是有些会议没有简称。

------------------------------------2017.04.21---------------------------------
首先将citation数据库dump出来一份用于备份，放到本地~/dump目录下。
下面清空mysql数据库的paper表、paper表、a2p表。然后将aminer中数据经过一定处理后放入数据库。
目前venue和dblpname不匹配的：
2925	DALT'10 Proceedings of the 8th international conference on Declarative agent languages and technologies VIII	AAMAS	625

------------------------------------2017.04.22----------------------------------
决定将citation数据库的paper表、veunue表的dblpname,dblp_dblp_id、 清空，然后加入经过预处理的aminer数据。
aminer中paper需要预处理：
1. 去除paper_title==venue_name的；
2. 去除没有发表年份的；
3. 处理paper_title,去掉无关部分；
编写程序：Insert_Paper_extend.py

问题：mysql读写安全模式
解决：SET SQL_SAFE_UPDATES=0

重新插入paper，venue后，发现以下情况：
总共1343235篇2010-2014年的符合条件的论文，其中：
1. 98823篇论文没有摘要
2. 47篇论文的venue name为空
3. 23篇论文的paper title为空
因此我们需经过数据库处理，去除2,3两部分的数据。同时删除venue中的venue_name为空的（其venue_id='461'）。
其SQL语句如下：
USE citation;
DELETE FROM paper WHERE paper_title = '';
DELETE FROM paper WHERE venue_venue_id = '461';
DELETE FROM venue WHERE venue_id = '461';

-----------------------------------2017.04.24-------------------------------------
在ccf和core表中添加两列dblpname，用于记录多个dblp名。解决venue更名的问题。

调查dblp，发现有几种论文类型，如下所示：
1. Journal Articles:	<li class="entry article" />
2. Conference and Workshop Papers:	<li class="entry inproceedings" />
3. Informal and Other Publications:		<li class="entry informal" />
4. Books and Theses:	<li class="entry book" />

重写程序向venue中插入对应dblp名。
问题：程序莫名终止
解决：requests请求加上timeout字段。

----------------------------------2017.04.25--------------------------------------
考虑到速度原因，决定将程序改为多线程程序，并且在5台机器上开5个进程运行程序。
数据按照venue_id的大小，分为5部分给5个进程，每个进程开4个线程。
问题：cursor问题
解决：每个线程都定义一个新的cursor
该程序运行结束大概耗时3小时左右，速度提升接近20倍。

下面修改Crawl_paper程序，更改逻辑错误。
程序修改完成，但是由于CCF和CORE的dblpname问题，还需先爬取CCF的dblpname

----------------------------------2017.04.26--------------------------------------
问题：CCF的dblpname很多为NOT IN DBLP
解决：当前所找到的论文在dblp中不唯一，应该找到唯一的paper来确定dblpname

问题：assert断言发现可能dblpname_list < 1，发现一个例子是：
venue: Information and Software Technology
发现匹配的likely match为：
Likely matches
International Joint Conferences on Computer, Information, and Systems Sciences, and Engineering (CISSE)
但是这个就和venue完全不同， 因为程序未进行命名匹配，所以存在漏洞。
同时，发现CISSE既不是期刊也不是会议。（entry editor）
解决：捕获断言异常，赋dblpname为NOT IN DBLP。

core中dblpname也已经更新完成。下面对数据进行一点整理：
1. venue表中共有22288个venue，其中4472个venue为"NOT IN DBLP"， 936个venue末尾带括号，最后在匹配时候应该去掉括号；
	大概20%的venue不在dblp中
2. CCF表中共有572个venue（包含一个NOT IN CCF），其中38个venue为"NOT IN DBLP"， 6%的venue不在dblp中，
   有29个venue存在第二个dblp名（但是有些和第一个是一样的）， 有8个venue存在第三个dblp名。
3. CORE表中共有2566个venue（包含一个NOT IN CORE），其中379个venue为"NOT IN DBLP"，14%的venue不在dblp中，
   有44个venue存在第二个dblp名（但是有些和第一个是一样的）， 有12个venue存在第三个dblp名。
4. CCF表中：
	A: 68个，3个NOT IN DBLP
	B: 230个，10个NOT IN DBLP
	C: 273个，25个NOT IN DBLP
5. CORE表中：
	A*: 124个，14个NOT IN DBLP
	A : 387个， 49个NOT IN DBLP
	B : 679个， 99个NOT IN DBLP
	C : 1258个， 211个NOT IN DBLP
	Australasian: 78个（均为会议）
	其他：39个

下面将venue， ccf， core表中的所有dblpname放入dblp表中
应先将venue加入dblp表中，等爬完所有论文之后再统一扩充dblp表

----------------------------------2017.04.27----------------------------------------
完成了CORE、CCF与dblp数据库的对应，对应情况如下：
dlbp-ccf: CCF中共572个venue，有496组对应（其中包括458个正常对应和38个NOT IN DBLP）；
dblp-core：CORE中共2566个venue，有1485组对应（其中包括1106个正常对应和379个NOT IN DBLP）；

下面需要根据ccf的venue来爬取相应论文的引用。
经过查询，发现CCF中paper占总paper的81.93%。这还没考虑CORE中的venue，因此决定直接爬下所有论文的信息。总共1,343,165篇
这里可以采用之前爬venue的dblp名的方法，采用5台机器，每台机器开4个线程，总共20个线程同时爬

问题：paper中paper_id并不连续，不适合之前的并行处理方法。
解决：在paper表中添加index_id字段，该字段自增

问题：搜索论文名相同的有多篇论文，因此导致引用量出现错误。
解决： &as_ylo=2000&as_yhi=2000
加上时间限制：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?q=Debugging+via+run-time+type+checking&btnG=&hl=en&as_sdt=0%2C5&as_ylo=2000&as_yhi=2000
不加时间限制：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?q=Debugging+via+run-time+type+checking&hl=en&as_sdt=0,5


----------------------------------2017.05.02------------------------------------------
问题：连续爬虫导致网站被封
解决：加上sleep并随机停止

测试发现很多论文不能找到引用信息，应先将多线程停止，这样便于观察。

下面的一个重要工作是分析现有数据集的信息，对当前数据集应有一个清晰的认识。
先添加数据库的author和a2p两个表的内容。

最终得到
author数目为：171,2433（包括所有作者，可能有些作者的论文不在paper中）
paper数目为：134,3165
作者与paper关系数目为：378,9487

----------------------------------2017.05.04-----------------------------------------
删除论文不在paper表中的作者后
author数目为：127,7094（删除数目为435,339）

备份数据库的author表

问题：dblp2ccf表中数据有歧义，导致问题，
		比如CCF_id = 19， 该CCF venue是可以在dblp网站上查到的，但是在dblp表中没有，因此链接到NOT IN DBLP， 这一类有10个
		比如CCF_id = 140， 该CCF venue是不能在dblp网站上查到的，在dblp表中也没有，因此链接到NOT IN DBLP， 这一类有28个
解决：将CCF和CORE中的所有venue也放到dblp表中

----------------------------------2017.05.05--------------------------------------------
无论如何也解决不了反爬虫问题。
代理也不行

禁用浏览器cookie，观察Response Headers 和 Request Headers:
第一次：
https://www.xichuan.pub/scholar?hl=zh-CN&q=recurrent+neural+network&lr=&oq=recurrent
Response:
Set-Cookie:NID=102=fFpEjF6Pt3-jxpl-MvHJvtJDrIa98T_uQFuX5PyR17TlD2kzUNONOZMlvluupfBqmNvbKxt7s9rHAgSIKFZqA36KH-LLXUb1MBLX7Ongm5vV8nXHF1yPHotQg5m_TRWY; expires=Sat, 04-Nov-2017 14:03:18 GMT; path=/; domain=.www.xichuan.pub
Set-Cookie:GSP=LM=1493992998:NW=1:S=U3cQ42jHEcQnm64J; expires=Sun, 05-May-2019 14:03:18 GMT; path=/; domain=.www.xichuan.pub

Request:
Cookie:空

第二次
https://www.xichuan.pub/scholar?q=recurrent+neural+network+tutorial&hl=zh-CN&as_sdt=0%2C5&oq=recurrent+neural+network
Response:
Set-Cookie:NID=102=YZkQCasCKZuK2YXzyj7jHUk96PWYgoOa1yzOQriPQqwpf_D1Sag2suBSmn5A9QZ4ihwIu-h-kwhx5wZo7lDKYOhe6NiAHQx8yJpJnze8AwA9NZZ80Awdq75Zd4hxjUnY; expires=Sat, 04-Nov-2017 14:05:16 GMT; path=/; domain=.www.xichuan.pub
Set-Cookie:GSP=LM=1493993116:NW=1:S=aC5AndsAufIxu5Y1; expires=Sun, 05-May-2019 14:05:16 GMT; path=/; domain=.www.xichuan.pub

Request：
Cookie：空

第三次：
https://www.xichuan.pub/scholar?q=computer+simulation&hl=zh-CN&as_sdt=0%2C5&oq=computer+
Response:
Set-Cookie:NID=102=AKNVt64dxXX2mz4AB1sZD_cclvTyYbGDyB9N3kPl4yeKuHExzYPW-Mk87BfYl4yTBiS7XdPy_NXI3Bcmti1vIdytcyw-_6Qf9Gf9LISDOJgpMfHjLO2OW8K7tl7_gSi2; expires=Sat, 04-Nov-2017 14:06:14 GMT; path=/; domain=.www.xichuan.pub
Set-Cookie:GSP=LM=1493993174:NW=1:S=X_oXClDA3xGAnd8L; expires=Sun, 05-May-2019 14:06:14 GMT; path=/; domain=.www.xichuan.pub

Request:
Cookie:空


https://github.com/geekan/google-scholar-crawler
这是google scholar的爬虫，但是对我们程序意义不大。
找到了一个aminer-spider项目和说明，如下：
http://billy-inn.github.io/Homepage/Crawler%20For%20Google%20Scholar.pdf
https://github.com/neozhangthe1/aminer-spider
作者的github：
https://github.com/billy-inn/CrawlerForGoogleScholar
https://billy-inn.github.io/

----------------------------------2017.05.06--------------------------------------------
问题：静态Cookie会被封IP的问题
准备尝试使用

不禁用浏览器Cookie，观察结果
第一次：
Response:
Set-Cookie:NID=102=CU-wknFUNgzcZrg-C9ouRh4LzMsFrV-3Rhl2T_qf6rj7fY4Ofhm1FRsJ9w8RFMfwrKnkkw4N69uabPZVwUB_Eh6a-0F33IZeN3MsD4AlYpOJ7MGtspzfvxuOztQ3nQ46; expires=Sun, 05-Nov-2017 01:46:39 GMT; path=/; domain=.g.sci-hub.cn
Set-Cookie:GSP=LM=1494035199:NW=1:S=ZpKmGabAURKEYQAj; expires=Mon, 06-May-2019 01:46:39 GMT; path=/; domain=.g.sci-hub.cn

Request:
Cookie:空

第二次：
Set-Cookie:GSP=NW=1:LM=1494035255:S=ZvRNnr2rJJeM9TdV; expires=Mon, 06-May-2019 01:47:35 GMT; path=/; domain=.g.sci-hub.cn

Request:
Cookie:NID=102=CU-wknFUNgzcZrg-C9ouRh4LzMsFrV-3Rhl2T_qf6rj7fY4Ofhm1FRsJ9w8RFMfwrKnkkw4N69uabPZVwUB_Eh6a-0F33IZeN3MsD4AlYpOJ7MGtspzfvxuOztQ3nQ46; GSP=LM=1494035199:NW=1:S=ZpKmGabAURKEYQAj; Hm_lvt_a1ee2279175d79290813a36e213338e7=1494035192; Hm_lpvt_a1ee2279175d79290813a36e213338e7=1494035192

第三次：
Response:
Set-Cookie:空

Request:
Cookie:NID=102=CU-wknFUNgzcZrg-C9ouRh4LzMsFrV-3Rhl2T_qf6rj7fY4Ofhm1FRsJ9w8RFMfwrKnkkw4N69uabPZVwUB_Eh6a-0F33IZeN3MsD4AlYpOJ7MGtspzfvxuOztQ3nQ46; GSP=NW=1:LM=1494035255:S=ZvRNnr2rJJeM9TdV; Hm_lvt_a1ee2279175d79290813a36e213338e7=1494035192; Hm_lpvt_a1ee2279175d79290813a36e213338e7=1494035248

使用每50次更换一次cookie的方法顺利爬取了150次，下面尝试缩短等待时间， 成功运行500次则开启多线程

问题：访问xue.glgoo.com不用cookie，但是返回的cookie为空

尝试：不改cookie，删掉cookie，试一下效果
结果：只运行了4次就失败了。

有一种情况，加入删去cookie，就报错误，但是加上cookie就可以访问
对于浏览器，如果清空cookie，然后再刷新页面就需要验证。


###cookielib学习
CookieJar.extract_cookies(response, request)
从response中抽取cookie放到CookieJar中。
其实requests的res.cookies是获得的response headers中的cookies,如果想获得requests的cookie,应该使用res.request.headers

免费试用讯代理的代理IP效果很差


尝试：去掉timeout，定时重启程序，更换cookie和user_agent
结果：连续运行100多个也会被封。

使用test_http_proxy.py程序测试http代理的使用问题，发现代理失效速度很快，因此确实需要不断更换代理
下面准备向程序中引入代理

现在不断更换cookie和代理可以完成访问，但是可能访问量太大，导致代理服务器出现问题。


---------------------------------------------2017.05.07-------------------------------------------
在测试https://www.xichuan.pub/scholar时发现，本地无法访问，清空cookie也不行。
但是我们禁用cookie之后就可以继续访问。

---------------------------------------------2017.05.09--------------------------------------------
尝试修改程序，将常用参数放在一起

尝试利用自建google scholar镜像来测试是否能够爬取成功
发现二级爬虫用不了， 下面测试固定Cookie下的情况

成功运行了131次


--------------------------------------------2017.05.12---------------------------------------------
尝试添加Crawl_paper_foreign.py文件，尝试使用国外ip访问google。
发现只更换cookie但是不换ip 大概运行80次就会人机验证。（2次更换cookie）。问题有待解决。

--------------------------------------------2017.05.17---------------------------------------------
将aminer数据库引用关系进行了更新， 将aminer_gai数据库进行备份。更新格式类似我们所建数据库的形式。下面进行简单的统计

1. 基本信息：
	author: 1,277,094个作者信息
	paper: 1,343,165篇论文信息
	a2p: 3,789,487个作者与论文的关系
	relationship: 4,328,659个引用关系（A->B表示B论文引用了A论文， A的出度就是A的引文个数，但是不全）

	venue: 22,288个venue信息 
	dblp : 5,083个dblp信息（包含了ccf和dblp中在dblp中的全部信息）
	ccf : 571个ccf信息（其中24个不在dblp中）
	core : 2,566个core信息
	dblp2ccf: 594个关系，（其中24个ccf_CCF_id与NOT IN DBLP对应）
	dblp2core: 2,593个关系 （其中370个core_CORE_id与NOT IN DBLP 对应， 364个属于A*, A, B, C）

2. author表：（1,277,094个作者信息）
	1) 411,766个作者没有affiliation信息，占32.24%
	2) 2个作者名为空
	3) 2,164个作者没有tag信息，占0.17%
	4) 708,994个作者H因子为0， 占55.52%
	5) 每个作者至少都发了1篇论文，只发表1篇的共784,713个，占61.45%， 只发表2篇的共182,864个， 占14.32%。
	6) 作者最多发表论文数为：752篇，具体信息为：
		'1175581', 
		'STAFF', 
		'752', 
		'3', 
		'International Conference;Author Index Volume;Index Volume;Special Issue;Conference Calendars;European Conference;Ambient Intelligence;Information Science;German Conference;IEEE International Conference', 
		'Computer Standards & Interfaces;Springer London Ltd., UK;NONE;25–28 April 2000, University of Vienna, Austria;http://www.bcs‐sges.org/es2001/;http://www.kr.tuwien.ac.at/KI2001/;bogus;Office of the Privacy Commissioner'
		经过查找，发现该作者好像不是一个人，而且都是一些一看就不是论文的论文。

		排名第2多的作者发表论文：601篇， 具体信息为：(该作者存在)
		'234102', 
		'Jack J. Dongarra', 
		'601', 
		'33', 
		'high performance;performance analysis;performance result;good performance;performance data;high performance computing platform;performance issue;peak performance;performance penalty;collective communicationsThe performance', 
		'Argonne National Laboratory, Argonne, IL;Univ. of Tennessee, Knoxville;University of Tennessee;Oak Ridge National Laboratory and University of Tennessee;Computer Science and Engineering, Seoul National University, Seoul, 151742, Korea;DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE, UNIVERSITY OF                        TENNESSEE, KNOXVILLE, TENNESSEE, OAK RIDGE NATIONAL LABORATORY, OAK RIDGE,                        TE ...;Dept. of Elec. Eng. and Comp. Sci., Univ. of Tennessee and Comp. Sci. and Math. Div., Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A. and Univ. of Manchester, Manchester, U.K.;Computer Science Department, University of Tennessee, Kruislaan 403, Knoxville, TN, The Netherlands;Computer Science Department, University of Tennessee, Knoxville, TN, China;Computer Science Department, University of Tennessee, Altenbergerstraße 69, Knoxville, TN, Austria;Computer Science Department, University of Tennessee, Real Casa dell\'Annunziata  via Roma, Knoxville, TN, Italy;Computer Science Department, University of Tennessee, Kruislaan 403, Knoxville, SJ, The Netherlands;Computer Science Department, University of Tennessee, rue CharlesCamichel, Knoxville, TN, France;Computer Science Department, University of Tennessee, Kruislaan 403, TN, Knoxville, The Netherlands;Computer Science Department, University of Tennessee, Kruislaan 403, Knoxville, The Netherlands;Computer Science Department, University of Tennessee, Nordbergstr. 15/C/3, Knoxville, Caltech, Italy;dongarra@cs.utk.edu;Electrical Engineering and Computer Science Department, University of Tennessee, Knoxville, TN, Poland;Faculty of Computer Science, Research Group Scientific Computing, University of Tennessee, Währinger Str. 29/6.21, Knoxville, TN, Austria'
	7) 最高H因子作者为：60， 具体信息为：
		'1336062', 
		'Hector Garcia-Molina', 
		'408', 
		'60', 
		'data model;data warehousing;data cloud;data item;data replication;data source;relational data;source data;structured data;base data', 
		'Princeton Univ., Princeton, NJ;Princeton;Arvin Park;Department of Computer Science, Stanford University, Stanford, CA;Department of Computer Science, Stanford, CA;Dept. of Computer Science, Stanford, CA;Department of Computer Sciences, Stanford, CA;Stanford, CA;Serra Mall, Stanford, CA'
	8) affiliation中带有China的作者个数为：105,845个， 占8.29%
	9) affiliation中带有Australia的作者个数为：15,648个， 占1.23%
	10) affiliation中带有USA, CA, LA的作者个数为：469,933个，占36.80%
		（这还是不全的美国作者，可见数据集中大约有接近一半是美国作者。）

3. paper表（1,343,165篇论文信息）
	1) 引用数最多的论文，引用量为：3,873，具体信息为：（谷歌学术现在搜索的引用量为41643）
		'760805', 
		'Distinctive Image Features from Scale-Invariant Keypoints', 
		'2004', 
		'20', （这个是他的参考文献数，实际比这个要多，因为我们只考虑我们数据集内部的文章）
		'This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.', NULL, '131', NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
	2) 论文发表年份最小的为2000， 最大的为2014
	3) 论文引用量为0的有707,936篇论文， 占52.71%，引用量为1的有203,556， 占15.15%
	4) 论文摘要信息为空的有98,775篇论文， 占7.35%


下面有如下几个问题需进一步分析：
1. 每篇存在引用的论文每一年的引用量变化情况
2. 不同国家作者在ccf和venue交叉部分的平均论文发表量
3. 每一类交叉部分引用其他类交叉部分的情况


--------------------------------------------2017.05.18---------------------------------------------
问题：CCF中存在同名venue属于不同computer category的情况
解决：暂时不管，没有影响

问题：CORE中存在同名venue属于不同classification的情况
例如：
1. core_id: 3453， 1674， 既属于A, 又属于C
2. 2132, 699 B, C
3. 845, 3195 A B
4. 873, 2034 B C
5. 1048, 1990 A C
6. 2059, 1064 B C
7. 1552, 3684 C B (J/C)
8. 2086, 1577 B, C
9. 3453 1674 A, C (J/C)

待删core_id
1236
3594
59
995
2056
426
52
1207
787
1884
208
820
1340
2080
708
620





问题
ccf 538--IDC
core 995 2056 -- IDC
ccf 161--DSN
core 787 1884 -- DSN


根据dblp无法对应

完成上述删除之后发现没有update重复情况了，成功解决问题

下面准备在relationship表中加入一些字段，这些字段可能与mysql信息冗余，但是为了查找方便决定加入下列冗余信息：
src:
src_publicationYear, src_max_Hindex, src_label, src_country(0,1,2,3, 分别表示其他、中、澳、混合中澳)

dst:
dst_publicationYear, dst_max_Hindex, dst_label, dst_country(0,1,2,3, 分别表示其他、中、澳、混合中澳)

--------------------------------------------2017.05.19---------------------------------------------
本想在relationship中加入冗余信息， 但是可能信息量太大，导致添加速度缓慢，下面考虑在paper表中增加
paper_max_Hindex, paper_country两个字段

--------------------------------------------2017.05.24---------------------------------------------
更改paper表速度太慢，下面我们采用pandas进行一定的分析。

1. 在所有的1,343,165篇文章中， 有37,259篇文章没有作者
2. 最大H因子为0的论文篇数为186,017篇，最大H因子为60的论文篇数为177篇，最大H因此为0,1,2,3,4的论文数为676,509篇，占51.8%，超过一半
3. paper的第一作者是中国的论文数量为110,378
4. paper的第一作者是Australia的论文数量为32,095（发现有2446篇重复，去掉之后剩余29,649篇）
5. paper中有378,874篇论文在CCF中能检索到
6. paper中有593,045篇论文在CORE中能检索到
7. paper中有313,216篇论文可以被CCF和CORE交叉索引
8. 有2446篇论文的第一作者属于两个国家？比如paper_id = 336868, 第一作者信息如下：
	'MiAlbum - a system for home photo managemet using the semi-automatic image annotation approach', 
	'1', 
	'Liu Wenyin', 
	'Microsoft Research China, 49 Zhichun Road, Beijing 100080, China;Microsoft Research, Beijing, China;City University of Hong Kong, Kowloon, Hong Kong;Department of Computer Science, City University of Hong Kong Tat Chee Avenue, Hong Kong SAR, PRC;Sch. of Electr., Comput. & Telecommun. Eng., Wollongong Univ., NSW, Australia;IEEE;State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China and Department of Computer Science, City University of Hong Kong, Hong Kong SAR, PR China;University of Science and Technology of China, School of Computer Science and Technology, Hefei, China and City University of Hong Kong, Department of Computer Science, HKSAR, China and CityUUSTC ...;School of Computer and Information Engineering, Shanghai University of Electronic Power, Shanghai 200090, China and Department of Computer Science and Technology, University of Science and Technol ...;College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China'

	虽然通过人工识别可以看出该论文为中国作者， 但是为了避免影响，决定去除所有的2446篇有歧义的论文（但是发现不会去掉所有重复）
	还是随便去掉Australia的吧。。。
9. 相互引用情况分析：
	1) 图中共有4,328,659条引用关系
	2) 其中交叉部分引用关系数量为980,783条（源是2000年的共有77,715条）
10. 针对源是2000年的交叉部分引用关系情况分析、computer类别是8（人工智能）、源和目的都是Conference：(总共2347个引用关系)
	1) 先看下引用2000年交叉部分的论文的论文分别在交叉部分是怎么分布的。