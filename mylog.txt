This project is applied for data crawler from google scholar. And the next time we will predict the number of paper's citation.

Fllowing is my work everytime.

----------------------------------2017.4.10------------------------------
使用谷歌学术镜像网站
https://a.ggkai.men/extdomains/scholar.google.com/
设置内容为英文显示，每次显示20行

给出两篇论文的url格式：
1: Blowing in the wind: unanchored patient information work during cancer care

url1: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=Blowing+in+the+wind%3A+unanchored+patient+information+work+during+cancer+care&btnG=&as_sdt=1%2C5&as_sdtp= 

2: I don't mind being logged, but want to remain in control: a field study of mobile activity and context logging     
url2: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=I+don%27t+mind+being+logged%2C+but+want+to+remain+in+control%3A+a+field+study+of+mobile+activity+and+context+logging&btnG=&as_sdt=1%2C5&as_sdtp=

可以发现假设论文名字为PAPER NAME, url格式为：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=PAPER+NAME&btnG=&as_sdt=1%2C5&as_sdtp=

一定会存在一些论文在google学术里面无法找到，因此需要抓住关键信息，判断是否可以在google学术中搜索到。
使用requests, BeautifulSoup进行测试，如下：
response = requests.get(url,headers=headers)
soup = BeautifulSoup(response.text)

linkinfo = soup.find("div", {"class":"gs_a"}).get_text()
可以得到linkinfo是google学术条目的作者信息那一行，我们可以使用作者信息判断该论文是否可以在谷歌学术中找到。

问题：作者名字存在法语或其他特殊字符
解决：目前不需要该作者信息

-----------------------------------2017.4.11---------------------------------
写一个小demo测试爬虫是否正确
镜像网站不稳定，因此使用vpn访问https://scholar.google.com
记录headers:

cookie = 'NID=101=SZYmK1bDCNI9YVEM-lBxM975ArpgyelHkwNMiiCJVjoY4sbhBGUWJ-zzrlo2_r1-8LeeeavR1hn8UxP2MuAM92L-uWOzdhExx-OIZZhuVlAGDS6P7XpR15PzPlcPSErq; GSP=IN=7e6cc990821af63:LD=en:NR=20:LM=1491915063:S=OmZJEGX4GZgsoDj5'
headers = {
	'Host': 'scholar.google.com',
	'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
	'Accept-Encoding':'gzip, deflate, sdch, br',
	'Accept-Language':'zh-CN,en-US;q=0.8,zh;q=0.5,en;q=0.3',
	'Cookie': cookie,
	'Referer':'https://www.google.com',
	'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
	'Cache-Control':'max-age=0',
}


问题：google学术使用requests.utils.dict_from_cookiejar(response.cookies)没有cookie返回。
待解决。

---------------------------------2017.04.13------------------------------------
首先测试两次无headers爬取google的cookie分别是什么？
第一次访问：
https://scholar.google.com/scholar?hl=en&q=Yoshua+Bengio&as_sdt=1%2C5&as_sdtp=&oq=
最终cookie为：
'GSP': 'LM=1492085690:S=MT2DJ2tBDnQA2lmr'
'NID': '101=fSdLNDYt8Adg6J2jqn2zV1FLD1VtFzPPA1tZM52d7DWDhMxo8M43HgTBZ6Uw-54Hn5el6rJmHrJPjHbCdzBo1B2PpDGHoB2zLaZ3uOrz7TvoCt8EY_rQGxgK6t9xNtsH'

第二次访问：
https://scholar.google.com/scholar?q=computer+network+security&hl=en&as_sdt=0%2C5&oq=
最终cookie为：
'GSP': 'LM=1492085859:S=R3n1bWo54jcZMaNf',
'NID': '101=GascO37p3VoBYkwSB43S9c-Vuoo2Q4MoMCGFeW5E8OdG3Fmxh1BYhvyGeF2a37om1B-DMhhRmVSrWq_AyF6vmcVSGjz9Av0SACHAVsIU4b1DQucOUXaxTPzl8aDpzyUw'

决定继续写test爬虫，这次采用手工获取cookie和user-agent的方式，明天写代码。

---------------------------------2017.04.14------------------------------------
发现一个python库，user_agent, 可以生成user_agent,安装如下：
pip install -U user_agent
使用方法：
from user_agent import generate_user_agent
Agent = generate_user_agent()

---------------------------------2017.04.15------------------------------------
cookie不易获取， 准备尝试先爬代理， 然后用代理爬国内google学术的镜像。
发现一个requests的关键属性,比如：res = requests.get(url)
res.status_code为状态码，200为正常（requests.codes.ok）

从西刺代理网站爬取500个代理IP和端口号
问题：很多代理无法ping通。

---------------------------------2017.04.16------------------------------------
对Crawl_proxy:
写一个函数用于判断是否ip可以ping通，只有能够ping通的才会被写入proxies.txt中

问题：cmd下运行python出现SyntaxError错误
解决：因为有中文注释，第一行加上#encoding:utf-8

问题：代理用不了，报错：
 ProxyError: HTTPSConnectionPool(host='b.ggkai.men', port=443): Max retries exceeded with url: /extdomains/scholar.google.com/scholar?hl=en&q=Usable+gestures+for+mobile+interfaces%3A+evaluating+social+acceptability+&btnG=&as_sdt=1%2C5&as_sdtp= (Caused by ProxyError('Cannot connect to proxy.', error(10054, '')))


代理发现镜像网站好像没有反爬虫，先测试8000个左右例子，看是否有问题
发现大概爬100个左右会出现重定向次数过多的情况，需要清除cookie

梳理下现在的有待解决的问题：
1. 数据库表结构优化
2. paper加上一些属性，比如引用的url等
3. 数据库表内容的优化（直接去掉不必要内容）
4. dblp名字变更问题
5. 二级爬虫程序未写

在爬镜像网站时在headers中加入cookie，可以连续爬取，目前到第262个遇到问题，原因是ascii编码的问题
问题： 对于论文Designing urban media façades: cases and challenges，其中包括非ascii编码的字符
解决：编码问题，可以设置默认编码， 方式如下：
import sys
reload(sys)
sys.setdefaultencoding("utf-8")

demo跑了1056次，我主动停了demo，准备更新数据库。
先在demo中运行，首先在test_paper表中加入两列属性，一个是引用链接，另一个是pdf链接

Mysql语句为：
ALTER TABLE test_citation.test_paper ADD COLUMN paper_citationURL VARCHAR(2550);
ALTER TABLE test_citation.test_paper ADD COLUMN paper_pdfURL VARCHAR(2550);

添加Crawl_paper，用于真实数据库爬虫

-------------------------------------------2017.04.17------------------------------------------------
本来准备将所有论文的引用全部爬下来， 现在来看可以分开来爬， 按类来爬。 SQL程序如下：
#找出所有A类会议、未被爬过的的论文名
SELECT paper_id, paper_title, paper_nbCitation, paper_isseen
FROM citation.paper
WHERE venue_venue_id IN(
	SELECT venue_id#, venue_name, venue_dblpname
	FROM citation.venue
	WHERE dblp_dblp_id IN (
		SELECT dblp_id
		FROM citation.ccf, citation.dblp
		WHERE CCF_dblpname = dblp_name
		AND CCF_type = 'conference'
		AND CCF_classification = 'A'
	)
)
AND paper_nbCitation != -1

这里参数就是CCF_type 和CCF_classification

--------------------------------------------2017.04.17-----------------------------------------------
问题：爬虫程序阻塞，但是不会停止运行，这导致无法连续爬虫
解决：编写定时重启程序的脚本，主要是两步：杀死进程、启动进程
代码见code/restart.py


---------------------------------------------2017.04.19----------------------------------------------
问题：发现爬虫解析网页内容有逻辑问题。目前发现很多pdf链接爬取错误。主要原因是如果显示超过一个结果，则会找到第一个有pdf的pdf链接（即不一定是第一个）因此导致错误。虽然没有看引用数是否有问题，但是感觉逻辑上也可能有问题。
解决：暂时未解决。

现在工作大概有三部分：数据库、DBLP、Google Scholar
数据库：可能数据从Aminer移植过程中，由于去噪等逻辑漏洞，导致移植出错
DBLP：一文多投、一刊多名等问题
Scholar： 逻辑问题，try except块应该保证逻辑正确，可以多输出中间信息

同时现在脚本运行的日志记录有问题，比如第几条成功的情况下是不用记录的

准备将这三部分的数据错误问题解决一下再继续后面的工作。
首先是数据库
将钱学长给的全部数据的数据库citation_raw与我们数据库citation做比较，SQL代码如下：
SELECT citation.paper.paper_id, citation.paper.paper_title, citation.venue.venue_name, citation_raw.paper.paper_venue_name
FROM citation.paper, citation.venue, citation_raw.paper
WHERE citation.paper.paper_id = citation_raw.paper.paper_id
AND citation.paper.venue_venue_id = citation.venue.venue_id
AND citation.venue.venue_name != citation_raw.paper.paper_venue_name

最终得到结果，有486篇论文的venue_name不一样，使用Aminer数据库进行比较，最终得到：
####################例子1###################
'1401710', 'Panlingual lexical translation via probabilistic inference', 'Artificial Intelligence', 'Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010'

citation_raw：Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010
citation: Artificial Intelligence
AMiner : Artificial Intelligence
证明我们的数据库没有问题。
究其原因，我将该论文：Panlingual lexical translation via probabilistic inference放到DBLP数据库中搜索，得到结果如下：
found 2 matches
2010
	Mausam, Stephen Soderland, Oren Etzioni, Daniel S. Weld, Kobi Reiter, Michael Skinner, Marcus Sammer, Jeff A. Bilmes:
Panlingual lexical translation via probabilistic inference. Artif. Intell. 174(9-10): 619-637 (2010)
	Mausam, Stephen Soderland, Oren Etzioni:
Panlingual Lexical Translation via Probabilistic Inference. AAAI 2010
maintained by Schloss Dagstuhl LZI at University of Trier	homebrowsesearchabout
open data data released under the ODC-BY 1.0 license; see also our legal information page

可以发现该论文在2010年被放到了AI期刊和AIII会议。
##################例子2######################
'2078528', 'Proceedings of the 15th Eurographics Conference on Visualization', 'EuroVis \'13 Proceedings of the 15th Eurographics Conference on Visualization', 'Computer Graphics Forum'

citation_raw : Computer Graphics Forum
citation : EuroVis '13 Proceedings of the 15th Eurographics Conference on Visualization
AMiner : Proceedings of the 15th Eurographics Conference on Visualization
citation是正确的，并且citation在插入数据过程中进行了一定的正则处理

#################例子3#########################
'1381004', 'Decoupled Linear Estimation of Affine Geometric Deformations and Nonlinear Intensity Transformations of Images', '', 'IEEE Transactions on Pattern Analysis and Machine Intelligence'

citation_raw : IEEE Transactions on Pattern Analysis and Machine Intelligence
citation : 空
AMiner : 空
citation是正确的

#############################################
总结：
我们发现citation在venue栏应该是没有问题的， 学长的citation_raw应该是采用了别的方法得到的venue。
下面构建一个AMiner数据库，其中就包括两个表，一个是paper表，一个是author表。
从citation中把表结构复制过来（空表）, SQL代码如下
CREATE TABLE aminer.paper LIKE citation.paper;
CREATE TABLE aminer.author LIKE citation.author;

结合程序Insert_Paper.py 和 Insert_Author2.py来编写程序，将所有aminer信息原封不动写入aminer数据库
需要添加venue_name列，移出一些列，SQL代码如下：

#删除列
ALTER TABLE aminer.paper DROP COLUMN venue_venue_id;
ALTER TABLE aminer.paper DROP COLUMN paper_isseen;
ALTER TABLE aminer.paper DROP COLUMN paper_citationURL;
ALTER TABLE aminer.paper DROP COLUMN paper_pdfURL;
ALTER TABLE aminer.paper DROP COLUMN paper_nbCitation;
#添加列
ALTER TABLE aminer.paper ADD COLUMN paper_venuename VARCHAR(2550);

下面准备把citation数据库中author 和 a2p加入数据库

------------------------------------2017.04.20---------------------------------
讨论目前存在的问题：
CCF的DBLP名的爬虫问题：
1. Likely matches, 有些情况是venue换名字，有些匹配的是错的，需要判断。但是Exact match是一定对的。
2. Likely matches情况，如果venue换名字，应在mysql中加上两列，记录曾用名。
3. 期刊的dblp名不是简称，但会议的dblp名应该是简称，需要再验证下。但是有些会议没有简称。

------------------------------------2017.04.21---------------------------------
首先将citation数据库dump出来一份用于备份，放到本地~/dump目录下。
下面清空mysql数据库的paper表、paper表、a2p表。然后将aminer中数据经过一定处理后放入数据库。
目前venue和dblpname不匹配的：
2925	DALT'10 Proceedings of the 8th international conference on Declarative agent languages and technologies VIII	AAMAS	625

------------------------------------2017.04.22----------------------------------
决定将citation数据库的paper表、veunue表的dblpname,dblp_dblp_id、 清空，然后加入经过预处理的aminer数据。
aminer中paper需要预处理：
1. 去除paper_title==venue_name的；
2. 去除没有发表年份的；
3. 处理paper_title,去掉无关部分；
编写程序：Insert_Paper_extend.py

问题：mysql读写安全模式
解决：SET SQL_SAFE_UPDATES=0

重新插入paper，venue后，发现以下情况：
总共1343235篇2010-2014年的符合条件的论文，其中：
1. 98823篇论文没有摘要
2. 47篇论文的venue name为空
3. 23篇论文的paper title为空
因此我们需经过数据库处理，去除2,3两部分的数据。同时删除venue中的venue_name为空的（其venue_id='461'）。
其SQL语句如下：
USE citation;
DELETE FROM paper WHERE paper_title = '';
DELETE FROM paper WHERE venue_venue_id = '461';
DELETE FROM venue WHERE venue_id = '461';

-----------------------------------2017.04.24-------------------------------------
在ccf和core表中添加两列dblpname，用于记录多个dblp名。解决venue更名的问题。

调查dblp，发现有几种论文类型，如下所示：
1. Journal Articles:	<li class="entry article" />
2. Conference and Workshop Papers:	<li class="entry inproceedings" />
3. Informal and Other Publications:		<li class="entry informal" />
4. Books and Theses:	<li class="entry book" />

重写程序向venue中插入对应dblp名。
问题：程序莫名终止
解决：requests请求加上timeout字段。

----------------------------------2017.04.25--------------------------------------
考虑到速度原因，决定将程序改为多线程程序，并且在5台机器上开5个进程运行程序。
数据按照venue_id的大小，分为5部分给5个进程，每个进程开4个线程。

下面修改Crawl_paper程序，更改逻辑错误。
