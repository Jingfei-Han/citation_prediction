This project is applied for data crawler from google scholar. And the next time we will predict the number of paper's citation.

Fllowing is my work everytime.

----------------------------------2017.4.10------------------------------
使用谷歌学术镜像网站
https://a.ggkai.men/extdomains/scholar.google.com/
设置内容为英文显示，每次显示20行

给出两篇论文的url格式：
1: Blowing in the wind: unanchored patient information work during cancer care

url1: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=Blowing+in+the+wind%3A+unanchored+patient+information+work+during+cancer+care&btnG=&as_sdt=1%2C5&as_sdtp= 

2: I don't mind being logged, but want to remain in control: a field study of mobile activity and context logging     
url2: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=I+don%27t+mind+being+logged%2C+but+want+to+remain+in+control%3A+a+field+study+of+mobile+activity+and+context+logging&btnG=&as_sdt=1%2C5&as_sdtp=

可以发现假设论文名字为PAPER NAME, url格式为：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=PAPER+NAME&btnG=&as_sdt=1%2C5&as_sdtp=

一定会存在一些论文在google学术里面无法找到，因此需要抓住关键信息，判断是否可以在google学术中搜索到。
使用requests, BeautifulSoup进行测试，如下：
response = requests.get(url,headers=headers)
soup = BeautifulSoup(response.text)

linkinfo = soup.find("div", {"class":"gs_a"}).get_text()
可以得到linkinfo是google学术条目的作者信息那一行，我们可以使用作者信息判断该论文是否可以在谷歌学术中找到。

问题：作者名字存在法语或其他特殊字符
解决：目前不需要该作者信息

-----------------------------------2017.4.11---------------------------------
写一个小demo测试爬虫是否正确
镜像网站不稳定，因此使用vpn访问https://scholar.google.com
记录headers:

cookie = 'NID=101=SZYmK1bDCNI9YVEM-lBxM975ArpgyelHkwNMiiCJVjoY4sbhBGUWJ-zzrlo2_r1-8LeeeavR1hn8UxP2MuAM92L-uWOzdhExx-OIZZhuVlAGDS6P7XpR15PzPlcPSErq; GSP=IN=7e6cc990821af63:LD=en:NR=20:LM=1491915063:S=OmZJEGX4GZgsoDj5'
headers = {
	'Host': 'scholar.google.com',
	'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
	'Accept-Encoding':'gzip, deflate, sdch, br',
	'Accept-Language':'zh-CN,en-US;q=0.8,zh;q=0.5,en;q=0.3',
	'Cookie': cookie,
	'Referer':'https://www.google.com',
	'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
	'Cache-Control':'max-age=0',
}


问题：google学术使用requests.utils.dict_from_cookiejar(response.cookies)没有cookie返回。
待解决。

---------------------------------2017.04.13------------------------------------
首先测试两次无headers爬取google的cookie分别是什么？
第一次访问：
https://scholar.google.com/scholar?hl=en&q=Yoshua+Bengio&as_sdt=1%2C5&as_sdtp=&oq=
最终cookie为：
'GSP': 'LM=1492085690:S=MT2DJ2tBDnQA2lmr'
'NID': '101=fSdLNDYt8Adg6J2jqn2zV1FLD1VtFzPPA1tZM52d7DWDhMxo8M43HgTBZ6Uw-54Hn5el6rJmHrJPjHbCdzBo1B2PpDGHoB2zLaZ3uOrz7TvoCt8EY_rQGxgK6t9xNtsH'

第二次访问：
https://scholar.google.com/scholar?q=computer+network+security&hl=en&as_sdt=0%2C5&oq=
最终cookie为：
'GSP': 'LM=1492085859:S=R3n1bWo54jcZMaNf',
'NID': '101=GascO37p3VoBYkwSB43S9c-Vuoo2Q4MoMCGFeW5E8OdG3Fmxh1BYhvyGeF2a37om1B-DMhhRmVSrWq_AyF6vmcVSGjz9Av0SACHAVsIU4b1DQucOUXaxTPzl8aDpzyUw'

决定继续写test爬虫，这次采用手工获取cookie和user-agent的方式，明天写代码。

---------------------------------2017.04.14------------------------------------
发现一个python库，user_agent, 可以生成user_agent,安装如下：
pip install -U user_agent
使用方法：
from user_agent import generate_user_agent
Agent = generate_user_agent()

---------------------------------2017.04.15------------------------------------
cookie不易获取， 准备尝试先爬代理， 然后用代理爬国内google学术的镜像。
发现一个requests的关键属性,比如：res = requests.get(url)
res.status_code为状态码，200为正常（requests.codes.ok）

从西刺代理网站爬取500个代理IP和端口号
问题：很多代理无法ping通。

---------------------------------2017.04.16------------------------------------
对Crawl_proxy:
写一个函数用于判断是否ip可以ping通，只有能够ping通的才会被写入proxies.txt中

问题：cmd下运行python出现SyntaxError错误
解决：因为有中文注释，第一行加上#encoding:utf-8

问题：代理用不了，报错：
 ProxyError: HTTPSConnectionPool(host='b.ggkai.men', port=443): Max retries exceeded with url: /extdomains/scholar.google.com/scholar?hl=en&q=Usable+gestures+for+mobile+interfaces%3A+evaluating+social+acceptability+&btnG=&as_sdt=1%2C5&as_sdtp= (Caused by ProxyError('Cannot connect to proxy.', error(10054, '')))


代理发现镜像网站好像没有反爬虫，先测试8000个左右例子，看是否有问题
发现大概爬100个左右会出现重定向次数过多的情况，需要清除cookie

梳理下现在的有待解决的问题：
1. 数据库表结构优化
2. paper加上一些属性，比如引用的url等
3. 数据库表内容的优化（直接去掉不必要内容）
4. dblp名字变更问题
5. 二级爬虫程序未写

在爬镜像网站时在headers中加入cookie，可以连续爬取，目前到第262个遇到问题，原因是ascii编码的问题
问题： 对于论文Designing urban media façades: cases and challenges，其中包括非ascii编码的字符
解决：编码问题，可以设置默认编码， 方式如下：
import sys
reload(sys)
sys.setdefaultencoding("utf-8")

demo跑了1056次，我主动停了demo，准备更新数据库。
先在demo中运行，首先在test_paper表中加入两列属性，一个是引用链接，另一个是pdf链接

Mysql语句为：
ALTER TABLE test_citation.test_paper ADD COLUMN paper_citationURL VARCHAR(2550);
ALTER TABLE test_citation.test_paper ADD COLUMN paper_pdfURL VARCHAR(2550);

添加Crawl_paper，用于真实数据库爬虫

-------------------------------------------2017.04.17------------------------------------------------
本来准备将所有论文的引用全部爬下来， 现在来看可以分开来爬， 按类来爬。 SQL程序如下：
#找出所有A类会议、未被爬过的的论文名
SELECT paper_id, paper_title, paper_nbCitation, paper_isseen
FROM citation.paper
WHERE venue_venue_id IN(
	SELECT venue_id#, venue_name, venue_dblpname
	FROM citation.venue
	WHERE dblp_dblp_id IN (
		SELECT dblp_id
		FROM citation.ccf, citation.dblp
		WHERE CCF_dblpname = dblp_name
		AND CCF_type = 'conference'
		AND CCF_classification = 'A'
	)
)
AND paper_nbCitation != -1

这里参数就是CCF_type 和CCF_classification

--------------------------------------------2017.04.17-----------------------------------------------
问题：爬虫程序阻塞，但是不会停止运行，这导致无法连续爬虫
解决：编写定时重启程序的脚本，主要是两步：杀死进程、启动进程
代码见code/restart.py


---------------------------------------------2017.04.19----------------------------------------------
问题：发现爬虫解析网页内容有逻辑问题。目前发现很多pdf链接爬取错误。主要原因是如果显示超过一个结果，则会找到第一个有pdf的pdf链接（即不一定是第一个）因此导致错误。虽然没有看引用数是否有问题，但是感觉逻辑上也可能有问题。
解决：暂时未解决。

现在工作大概有三部分：数据库、DBLP、Google Scholar
数据库：可能数据从Aminer移植过程中，由于去噪等逻辑漏洞，导致移植出错
DBLP：一文多投、一刊多名等问题
Scholar： 逻辑问题，try except块应该保证逻辑正确，可以多输出中间信息

同时现在脚本运行的日志记录有问题，比如第几条成功的情况下是不用记录的

准备将这三部分的数据错误问题解决一下再继续后面的工作。
首先是数据库
将钱学长给的全部数据的数据库citation_raw与我们数据库citation做比较，SQL代码如下：
SELECT citation.paper.paper_id, citation.paper.paper_title, citation.venue.venue_name, citation_raw.paper.paper_venue_name
FROM citation.paper, citation.venue, citation_raw.paper
WHERE citation.paper.paper_id = citation_raw.paper.paper_id
AND citation.paper.venue_venue_id = citation.venue.venue_id
AND citation.venue.venue_name != citation_raw.paper.paper_venue_name

最终得到结果，有486篇论文的venue_name不一样，使用Aminer数据库进行比较，最终得到：
####################例子1###################
'1401710', 'Panlingual lexical translation via probabilistic inference', 'Artificial Intelligence', 'Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010'

citation_raw：Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010
citation: Artificial Intelligence
AMiner : Artificial Intelligence
证明我们的数据库没有问题。
究其原因，我将该论文：Panlingual lexical translation via probabilistic inference放到DBLP数据库中搜索，得到结果如下：
found 2 matches
2010
	Mausam, Stephen Soderland, Oren Etzioni, Daniel S. Weld, Kobi Reiter, Michael Skinner, Marcus Sammer, Jeff A. Bilmes:
Panlingual lexical translation via probabilistic inference. Artif. Intell. 174(9-10): 619-637 (2010)
	Mausam, Stephen Soderland, Oren Etzioni:
Panlingual Lexical Translation via Probabilistic Inference. AAAI 2010
maintained by Schloss Dagstuhl LZI at University of Trier	homebrowsesearchabout
open data data released under the ODC-BY 1.0 license; see also our legal information page

可以发现该论文在2010年被放到了AI期刊和AIII会议。
##################例子2######################
'2078528', 'Proceedings of the 15th Eurographics Conference on Visualization', 'EuroVis \'13 Proceedings of the 15th Eurographics Conference on Visualization', 'Computer Graphics Forum'

citation_raw : Computer Graphics Forum
citation : EuroVis '13 Proceedings of the 15th Eurographics Conference on Visualization
AMiner : Proceedings of the 15th Eurographics Conference on Visualization
citation是正确的，并且citation在插入数据过程中进行了一定的正则处理

#################例子3#########################
'1381004', 'Decoupled Linear Estimation of Affine Geometric Deformations and Nonlinear Intensity Transformations of Images', '', 'IEEE Transactions on Pattern Analysis and Machine Intelligence'

citation_raw : IEEE Transactions on Pattern Analysis and Machine Intelligence
citation : 空
AMiner : 空
citation是正确的

#############################################
总结：
我们发现citation在venue栏应该是没有问题的， 学长的citation_raw应该是采用了别的方法得到的venue。
下面构建一个AMiner数据库，其中就包括两个表，一个是paper表，一个是author表。
从citation中把表结构复制过来（空表）, SQL代码如下
CREATE TABLE aminer.paper LIKE citation.paper;
CREATE TABLE aminer.author LIKE citation.author;

结合程序Insert_Paper.py 和 Insert_Author2.py来编写程序，将所有aminer信息原封不动写入aminer数据库
需要添加venue_name列，移出一些列，SQL代码如下：

#删除列
ALTER TABLE aminer.paper DROP COLUMN venue_venue_id;
ALTER TABLE aminer.paper DROP COLUMN paper_isseen;
ALTER TABLE aminer.paper DROP COLUMN paper_citationURL;
ALTER TABLE aminer.paper DROP COLUMN paper_pdfURL;
ALTER TABLE aminer.paper DROP COLUMN paper_nbCitation;
#添加列
ALTER TABLE aminer.paper ADD COLUMN paper_venuename VARCHAR(2550);

下面准备把citation数据库中author 和 a2p加入数据库

------------------------------------2017.04.20---------------------------------
讨论目前存在的问题：
CCF的DBLP名的爬虫问题：
1. Likely matches, 有些情况是venue换名字，有些匹配的是错的，需要判断。但是Exact match是一定对的。
2. Likely matches情况，如果venue换名字，应在mysql中加上两列，记录曾用名。
3. 期刊的dblp名不是简称，但会议的dblp名应该是简称，需要再验证下。但是有些会议没有简称。

------------------------------------2017.04.21---------------------------------
首先将citation数据库dump出来一份用于备份，放到本地~/dump目录下。
下面清空mysql数据库的paper表、paper表、a2p表。然后将aminer中数据经过一定处理后放入数据库。
目前venue和dblpname不匹配的：
2925	DALT'10 Proceedings of the 8th international conference on Declarative agent languages and technologies VIII	AAMAS	625

------------------------------------2017.04.22----------------------------------
决定将citation数据库的paper表、veunue表的dblpname,dblp_dblp_id、 清空，然后加入经过预处理的aminer数据。
aminer中paper需要预处理：
1. 去除paper_title==venue_name的；
2. 去除没有发表年份的；
3. 处理paper_title,去掉无关部分；
编写程序：Insert_Paper_extend.py

问题：mysql读写安全模式
解决：SET SQL_SAFE_UPDATES=0

重新插入paper，venue后，发现以下情况：
总共1343235篇2010-2014年的符合条件的论文，其中：
1. 98823篇论文没有摘要
2. 47篇论文的venue name为空
3. 23篇论文的paper title为空
因此我们需经过数据库处理，去除2,3两部分的数据。同时删除venue中的venue_name为空的（其venue_id='461'）。
其SQL语句如下：
USE citation;
DELETE FROM paper WHERE paper_title = '';
DELETE FROM paper WHERE venue_venue_id = '461';
DELETE FROM venue WHERE venue_id = '461';

-----------------------------------2017.04.24-------------------------------------
在ccf和core表中添加两列dblpname，用于记录多个dblp名。解决venue更名的问题。

调查dblp，发现有几种论文类型，如下所示：
1. Journal Articles:	<li class="entry article" />
2. Conference and Workshop Papers:	<li class="entry inproceedings" />
3. Informal and Other Publications:		<li class="entry informal" />
4. Books and Theses:	<li class="entry book" />

重写程序向venue中插入对应dblp名。
问题：程序莫名终止
解决：requests请求加上timeout字段。

----------------------------------2017.04.25--------------------------------------
考虑到速度原因，决定将程序改为多线程程序，并且在5台机器上开5个进程运行程序。
数据按照venue_id的大小，分为5部分给5个进程，每个进程开4个线程。
问题：cursor问题
解决：每个线程都定义一个新的cursor
该程序运行结束大概耗时3小时左右，速度提升接近20倍。

下面修改Crawl_paper程序，更改逻辑错误。
程序修改完成，但是由于CCF和CORE的dblpname问题，还需先爬取CCF的dblpname

----------------------------------2017.04.26--------------------------------------
问题：CCF的dblpname很多为NOT IN DBLP
解决：当前所找到的论文在dblp中不唯一，应该找到唯一的paper来确定dblpname

问题：assert断言发现可能dblpname_list < 1，发现一个例子是：
venue: Information and Software Technology
发现匹配的likely match为：
Likely matches
International Joint Conferences on Computer, Information, and Systems Sciences, and Engineering (CISSE)
但是这个就和venue完全不同， 因为程序未进行命名匹配，所以存在漏洞。
同时，发现CISSE既不是期刊也不是会议。（entry editor）
解决：捕获断言异常，赋dblpname为NOT IN DBLP。

core中dblpname也已经更新完成。下面对数据进行一点整理：
1. venue表中共有22288个venue，其中4472个venue为"NOT IN DBLP"， 936个venue末尾带括号，最后在匹配时候应该去掉括号；
	大概20%的venue不在dblp中
2. CCF表中共有572个venue（包含一个NOT IN CCF），其中38个venue为"NOT IN DBLP"， 6%的venue不在dblp中，
   有29个venue存在第二个dblp名（但是有些和第一个是一样的）， 有8个venue存在第三个dblp名。
3. CORE表中共有2566个venue（包含一个NOT IN CORE），其中379个venue为"NOT IN DBLP"，14%的venue不在dblp中，
   有44个venue存在第二个dblp名（但是有些和第一个是一样的）， 有12个venue存在第三个dblp名。
4. CCF表中：
	A: 68个，3个NOT IN DBLP
	B: 230个，10个NOT IN DBLP
	C: 273个，25个NOT IN DBLP
5. CORE表中：
	A*: 124个，14个NOT IN DBLP
	A : 387个， 49个NOT IN DBLP
	B : 679个， 99个NOT IN DBLP
	C : 1258个， 211个NOT IN DBLP
	Australasian: 78个（均为会议）
	其他：39个

下面将venue， ccf， core表中的所有dblpname放入dblp表中
应先将venue加入dblp表中，等爬完所有论文之后再统一扩充dblp表

----------------------------------2017.04.27----------------------------------------
完成了CORE、CCF与dblp数据库的对应，对应情况如下：
dlbp-ccf: CCF中共572个venue，有496组对应（其中包括458个正常对应和38个NOT IN DBLP）；
dblp-core：CORE中共2566个venue，有1485组对应（其中包括1106个正常对应和379个NOT IN DBLP）；

下面需要根据ccf的venue来爬取相应论文的引用。
经过查询，发现CCF中paper占总paper的81.93%。这还没考虑CORE中的venue，因此决定直接爬下所有论文的信息。总共1,343,165篇
这里可以采用之前爬venue的dblp名的方法，采用5台机器，每台机器开4个线程，总共20个线程同时爬

问题：paper中paper_id并不连续，不适合之前的并行处理方法。
解决：在paper表中添加index_id字段，该字段自增

问题：搜索论文名相同的有多篇论文，因此导致引用量出现错误。
解决： &as_ylo=2000&as_yhi=2000
加上时间限制：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?q=Debugging+via+run-time+type+checking&btnG=&hl=en&as_sdt=0%2C5&as_ylo=2000&as_yhi=2000
不加时间限制：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?q=Debugging+via+run-time+type+checking&hl=en&as_sdt=0,5


----------------------------------2017.05.02------------------------------------------
问题：连续爬虫导致网站被封
解决：加上sleep并随机停止

测试发现很多论文不能找到引用信息，应先将多线程停止，这样便于观察。

下面的一个重要工作是分析现有数据集的信息，对当前数据集应有一个清晰的认识。
先添加数据库的author和a2p两个表的内容。

最终得到
author数目为：171,2433（包括所有作者，可能有些作者的论文不在paper中）
paper数目为：134,3165
作者与paper关系数目为：378,9487

----------------------------------2017.05.04-----------------------------------------
删除论文不在paper表中的作者后
author数目为：127,7094（删除数目为435,339）

备份数据库的author表

问题：dblp2ccf表中数据有歧义，导致问题，
		比如CCF_id = 19， 该CCF venue是可以在dblp网站上查到的，但是在dblp表中没有，因此链接到NOT IN DBLP， 这一类有10个
		比如CCF_id = 140， 该CCF venue是不能在dblp网站上查到的，在dblp表中也没有，因此链接到NOT IN DBLP， 这一类有28个
解决：将CCF和CORE中的所有venue也放到dblp表中

----------------------------------2017.05.05--------------------------------------------
无论如何也解决不了反爬虫问题。
代理也不行

禁用浏览器cookie，观察Response Headers 和 Request Headers:
第一次：
https://www.xichuan.pub/scholar?hl=zh-CN&q=recurrent+neural+network&lr=&oq=recurrent
Response:
Set-Cookie:NID=102=fFpEjF6Pt3-jxpl-MvHJvtJDrIa98T_uQFuX5PyR17TlD2kzUNONOZMlvluupfBqmNvbKxt7s9rHAgSIKFZqA36KH-LLXUb1MBLX7Ongm5vV8nXHF1yPHotQg5m_TRWY; expires=Sat, 04-Nov-2017 14:03:18 GMT; path=/; domain=.www.xichuan.pub
Set-Cookie:GSP=LM=1493992998:NW=1:S=U3cQ42jHEcQnm64J; expires=Sun, 05-May-2019 14:03:18 GMT; path=/; domain=.www.xichuan.pub

Request:
Cookie:空

第二次
https://www.xichuan.pub/scholar?q=recurrent+neural+network+tutorial&hl=zh-CN&as_sdt=0%2C5&oq=recurrent+neural+network
Response:
Set-Cookie:NID=102=YZkQCasCKZuK2YXzyj7jHUk96PWYgoOa1yzOQriPQqwpf_D1Sag2suBSmn5A9QZ4ihwIu-h-kwhx5wZo7lDKYOhe6NiAHQx8yJpJnze8AwA9NZZ80Awdq75Zd4hxjUnY; expires=Sat, 04-Nov-2017 14:05:16 GMT; path=/; domain=.www.xichuan.pub
Set-Cookie:GSP=LM=1493993116:NW=1:S=aC5AndsAufIxu5Y1; expires=Sun, 05-May-2019 14:05:16 GMT; path=/; domain=.www.xichuan.pub

Request：
Cookie：空

第三次：
https://www.xichuan.pub/scholar?q=computer+simulation&hl=zh-CN&as_sdt=0%2C5&oq=computer+
Response:
Set-Cookie:NID=102=AKNVt64dxXX2mz4AB1sZD_cclvTyYbGDyB9N3kPl4yeKuHExzYPW-Mk87BfYl4yTBiS7XdPy_NXI3Bcmti1vIdytcyw-_6Qf9Gf9LISDOJgpMfHjLO2OW8K7tl7_gSi2; expires=Sat, 04-Nov-2017 14:06:14 GMT; path=/; domain=.www.xichuan.pub
Set-Cookie:GSP=LM=1493993174:NW=1:S=X_oXClDA3xGAnd8L; expires=Sun, 05-May-2019 14:06:14 GMT; path=/; domain=.www.xichuan.pub

Request:
Cookie:空


https://github.com/geekan/google-scholar-crawler
这是google scholar的爬虫，但是对我们程序意义不大。
找到了一个aminer-spider项目和说明，如下：
http://billy-inn.github.io/Homepage/Crawler%20For%20Google%20Scholar.pdf
https://github.com/neozhangthe1/aminer-spider
作者的github：
https://github.com/billy-inn/CrawlerForGoogleScholar
https://billy-inn.github.io/

----------------------------------2017.05.06--------------------------------------------
问题：静态Cookie会被封IP的问题
准备尝试使用

不禁用浏览器Cookie，观察结果
第一次：
Response:
Set-Cookie:NID=102=CU-wknFUNgzcZrg-C9ouRh4LzMsFrV-3Rhl2T_qf6rj7fY4Ofhm1FRsJ9w8RFMfwrKnkkw4N69uabPZVwUB_Eh6a-0F33IZeN3MsD4AlYpOJ7MGtspzfvxuOztQ3nQ46; expires=Sun, 05-Nov-2017 01:46:39 GMT; path=/; domain=.g.sci-hub.cn
Set-Cookie:GSP=LM=1494035199:NW=1:S=ZpKmGabAURKEYQAj; expires=Mon, 06-May-2019 01:46:39 GMT; path=/; domain=.g.sci-hub.cn

Request:
Cookie:空

第二次：
Set-Cookie:GSP=NW=1:LM=1494035255:S=ZvRNnr2rJJeM9TdV; expires=Mon, 06-May-2019 01:47:35 GMT; path=/; domain=.g.sci-hub.cn

Request:
Cookie:NID=102=CU-wknFUNgzcZrg-C9ouRh4LzMsFrV-3Rhl2T_qf6rj7fY4Ofhm1FRsJ9w8RFMfwrKnkkw4N69uabPZVwUB_Eh6a-0F33IZeN3MsD4AlYpOJ7MGtspzfvxuOztQ3nQ46; GSP=LM=1494035199:NW=1:S=ZpKmGabAURKEYQAj; Hm_lvt_a1ee2279175d79290813a36e213338e7=1494035192; Hm_lpvt_a1ee2279175d79290813a36e213338e7=1494035192

第三次：
Response:
Set-Cookie:空

Request:
Cookie:NID=102=CU-wknFUNgzcZrg-C9ouRh4LzMsFrV-3Rhl2T_qf6rj7fY4Ofhm1FRsJ9w8RFMfwrKnkkw4N69uabPZVwUB_Eh6a-0F33IZeN3MsD4AlYpOJ7MGtspzfvxuOztQ3nQ46; GSP=NW=1:LM=1494035255:S=ZvRNnr2rJJeM9TdV; Hm_lvt_a1ee2279175d79290813a36e213338e7=1494035192; Hm_lpvt_a1ee2279175d79290813a36e213338e7=1494035248

使用每50次更换一次cookie的方法顺利爬取了150次，下面尝试缩短等待时间， 成功运行500次则开启多线程

问题：访问xue.glgoo.com不用cookie，但是返回的cookie为空

尝试：不改cookie，删掉cookie，试一下效果
结果：只运行了4次就失败了。

有一种情况，加入删去cookie，就报错误，但是加上cookie就可以访问
对于浏览器，如果清空cookie，然后再刷新页面就需要验证。


###cookielib学习
CookieJar.extract_cookies(response, request)
从response中抽取cookie放到CookieJar中。
其实requests的res.cookies是获得的response headers中的cookies,如果想获得requests的cookie,应该使用res.request.headers

免费试用讯代理的代理IP效果很差


尝试：去掉timeout，定时重启程序，更换cookie和user_agent
结果：连续运行100多个也会被封。

使用test_http_proxy.py程序测试http代理的使用问题，发现代理失效速度很快，因此确实需要不断更换代理
下面准备向程序中引入代理

现在不断更换cookie和代理可以完成访问，但是可能访问量太大，导致代理服务器出现问题。


---------------------------------------------2017.05.07-------------------------------------------
在测试https://www.xichuan.pub/scholar时发现，本地无法访问，清空cookie也不行。
但是我们禁用cookie之后就可以继续访问。

---------------------------------------------2017.05.09--------------------------------------------
尝试修改程序，将常用参数放在一起

尝试利用自建google scholar镜像来测试是否能够爬取成功
发现二级爬虫用不了， 下面测试固定Cookie下的情况

成功运行了131次


--------------------------------------------2017.05.12---------------------------------------------
尝试添加Crawl_paper_foreign.py文件，尝试使用国外ip访问google。
发现只更换cookie但是不换ip 大概运行80次就会人机验证。（2次更换cookie）。问题有待解决。