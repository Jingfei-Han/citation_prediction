This project is applied for data crawler from google scholar. And the next time we will predict the number of paper's citation.

Fllowing is my work everytime.

----------------------------------2017.4.10------------------------------
使用谷歌学术镜像网站
https://a.ggkai.men/extdomains/scholar.google.com/
设置内容为英文显示，每次显示20行

给出两篇论文的url格式：
1: Blowing in the wind: unanchored patient information work during cancer care

url1: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=Blowing+in+the+wind%3A+unanchored+patient+information+work+during+cancer+care&btnG=&as_sdt=1%2C5&as_sdtp= 

2: I don't mind being logged, but want to remain in control: a field study of mobile activity and context logging     
url2: https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=I+don%27t+mind+being+logged%2C+but+want+to+remain+in+control%3A+a+field+study+of+mobile+activity+and+context+logging&btnG=&as_sdt=1%2C5&as_sdtp=

可以发现假设论文名字为PAPER NAME, url格式为：
https://a.ggkai.men/extdomains/scholar.google.com/scholar?hl=en&q=PAPER+NAME&btnG=&as_sdt=1%2C5&as_sdtp=

一定会存在一些论文在google学术里面无法找到，因此需要抓住关键信息，判断是否可以在google学术中搜索到。
使用requests, BeautifulSoup进行测试，如下：
response = requests.get(url,headers=headers)
soup = BeautifulSoup(response.text)

linkinfo = soup.find("div", {"class":"gs_a"}).get_text()
可以得到linkinfo是google学术条目的作者信息那一行，我们可以使用作者信息判断该论文是否可以在谷歌学术中找到。

问题：作者名字存在法语或其他特殊字符
解决：目前不需要该作者信息

-----------------------------------2017.4.11---------------------------------
写一个小demo测试爬虫是否正确
镜像网站不稳定，因此使用vpn访问https://scholar.google.com
记录headers:

cookie = 'NID=101=SZYmK1bDCNI9YVEM-lBxM975ArpgyelHkwNMiiCJVjoY4sbhBGUWJ-zzrlo2_r1-8LeeeavR1hn8UxP2MuAM92L-uWOzdhExx-OIZZhuVlAGDS6P7XpR15PzPlcPSErq; GSP=IN=7e6cc990821af63:LD=en:NR=20:LM=1491915063:S=OmZJEGX4GZgsoDj5'
headers = {
	'Host': 'scholar.google.com',
	'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
	'Accept-Encoding':'gzip, deflate, sdch, br',
	'Accept-Language':'zh-CN,en-US;q=0.8,zh;q=0.5,en;q=0.3',
	'Cookie': cookie,
	'Referer':'https://www.google.com',
	'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
	'Cache-Control':'max-age=0',
}


问题：google学术使用requests.utils.dict_from_cookiejar(response.cookies)没有cookie返回。
待解决。

---------------------------------2017.04.13------------------------------------
首先测试两次无headers爬取google的cookie分别是什么？
第一次访问：
https://scholar.google.com/scholar?hl=en&q=Yoshua+Bengio&as_sdt=1%2C5&as_sdtp=&oq=
最终cookie为：
'GSP': 'LM=1492085690:S=MT2DJ2tBDnQA2lmr'
'NID': '101=fSdLNDYt8Adg6J2jqn2zV1FLD1VtFzPPA1tZM52d7DWDhMxo8M43HgTBZ6Uw-54Hn5el6rJmHrJPjHbCdzBo1B2PpDGHoB2zLaZ3uOrz7TvoCt8EY_rQGxgK6t9xNtsH'

第二次访问：
https://scholar.google.com/scholar?q=computer+network+security&hl=en&as_sdt=0%2C5&oq=
最终cookie为：
'GSP': 'LM=1492085859:S=R3n1bWo54jcZMaNf',
'NID': '101=GascO37p3VoBYkwSB43S9c-Vuoo2Q4MoMCGFeW5E8OdG3Fmxh1BYhvyGeF2a37om1B-DMhhRmVSrWq_AyF6vmcVSGjz9Av0SACHAVsIU4b1DQucOUXaxTPzl8aDpzyUw'

决定继续写test爬虫，这次采用手工获取cookie和user-agent的方式，明天写代码。

---------------------------------2017.04.14------------------------------------
发现一个python库，user_agent, 可以生成user_agent,安装如下：
pip install -U user_agent
使用方法：
from user_agent import generate_user_agent
Agent = generate_user_agent()

---------------------------------2017.04.15------------------------------------
cookie不易获取， 准备尝试先爬代理， 然后用代理爬国内google学术的镜像。
发现一个requests的关键属性,比如：res = requests.get(url)
res.status_code为状态码，200为正常（requests.codes.ok）

从西刺代理网站爬取500个代理IP和端口号
问题：很多代理无法ping通。

---------------------------------2017.04.16------------------------------------
对Crawl_proxy:
写一个函数用于判断是否ip可以ping通，只有能够ping通的才会被写入proxies.txt中

问题：cmd下运行python出现SyntaxError错误
解决：因为有中文注释，第一行加上#encoding:utf-8

问题：代理用不了，报错：
 ProxyError: HTTPSConnectionPool(host='b.ggkai.men', port=443): Max retries exceeded with url: /extdomains/scholar.google.com/scholar?hl=en&q=Usable+gestures+for+mobile+interfaces%3A+evaluating+social+acceptability+&btnG=&as_sdt=1%2C5&as_sdtp= (Caused by ProxyError('Cannot connect to proxy.', error(10054, '')))


代理发现镜像网站好像没有反爬虫，先测试8000个左右例子，看是否有问题
发现大概爬100个左右会出现重定向次数过多的情况，需要清除cookie

梳理下现在的有待解决的问题：
1. 数据库表结构优化
2. paper加上一些属性，比如引用的url等
3. 数据库表内容的优化（直接去掉不必要内容）
4. dblp名字变更问题
5. 二级爬虫程序未写

在爬镜像网站时在headers中加入cookie，可以连续爬取，目前到第262个遇到问题，原因是ascii编码的问题
问题： 对于论文Designing urban media façades: cases and challenges，其中包括非ascii编码的字符
解决：编码问题，可以设置默认编码， 方式如下：
import sys
reload(sys)
sys.setdefaultencoding("utf-8")

demo跑了1056次，我主动停了demo，准备更新数据库。
先在demo中运行，首先在test_paper表中加入两列属性，一个是引用链接，另一个是pdf链接

Mysql语句为：
ALTER TABLE test_citation.test_paper ADD COLUMN paper_citationURL VARCHAR(2550);
ALTER TABLE test_citation.test_paper ADD COLUMN paper_pdfURL VARCHAR(2550);

添加Crawl_paper，用于真实数据库爬虫